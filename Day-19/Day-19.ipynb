{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 19 Exercise\n",
    "### Exercise Level 1\n",
    "### Number 1a: Function to count number of lines and words in Obama speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in Obama speech are: 66\n",
      "The number of words in the Obama speech are: 2400\n"
     ]
    }
   ],
   "source": [
    "def count_lines_words(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "        line_count = len(lines)\n",
    "        words = ' '.join(lines).split()\n",
    "        word_count = len(words)\n",
    "    return line_count, word_count\n",
    "\n",
    "obama_lines, obama_words = count_lines_words(file_path= r'..\\data\\obama_speech.txt')\n",
    "print(f'The number of lines in Obama speech are: {obama_lines}')\n",
    "print(f'The number of words in the Obama speech are: {obama_words}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1b: Function to count number of lines and words in Michelle speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "michelle_lines, michelle_words = count_lines_words(file_path= r'..\\data\\michelle_obama_speech.txt')\n",
    "print(f'The number of lines in Michelle Obama speech are: {michelle_lines}')\n",
    "print(f'The number of words in Micheele Obama speech are: {michelle_words}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1c: Function to count number of lines and words in Donald speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "donald_lines, donald_words = count_lines_words(file_path= r'..\\data\\donald_speech.txt')\n",
    "print(f'The number of lines in Donald speech are: {donald_lines}')\n",
    "print(f'The number of words in Donald speech are: {donald_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 1d: Function to count number of lines and words in Melina Trump speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "melina_lines, melina_words = count_lines_words(file_path= r'..\\data\\melina_trump_speech.txt')\n",
    "print(f'The number of lines in Melina speech are: {melina_lines}')\n",
    "print(f'The number of words in Melina speech are: {melina_words}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 2: Finding ten most spoken countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def find_most_spoken_languages():\n",
    "    with open('../data/countries_data.json', 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    language_count = {}\n",
    "\n",
    "    for country in data:\n",
    "        languages = country['languages']\n",
    "\n",
    "        for language in languages:\n",
    "            if language in language_count:\n",
    "                language_count[language] += 1\n",
    "            else:\n",
    "                language_count[language] = 1\n",
    "\n",
    "    sorted_languages = sorted(language_count.items(), key=lambda x: x[1], reverse=True)\n",
    "    most_spoken_languages = [language for language, count in sorted_languages[:10]]\n",
    "\n",
    "    return most_spoken_languages\n",
    "\n",
    "most_spoken = find_most_spoken_languages()\n",
    "print(\"The Ten most spoken languages are:\")\n",
    "for language in most_spoken:\n",
    "    print(language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3: Function that creates a list of 10 most populated countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_populated_countries(filename, limit):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        sorted_countries = sorted(data, key=lambda x: x['population'], reverse=True)\n",
    "        return [{'country': country['name'], 'population': country['population']} for country in sorted_countries[:limit]]\n",
    "        \n",
    "    \n",
    "filename = r'../data/countries_data.json'\n",
    "print('The ten most populated countries are:')\n",
    "print(most_populated_countries(filename, 10))\n",
    "#print(most_populated_countries(filename, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise Level 2\n",
    "### Number 1: Extracting all incoming email addresses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_emails(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "    return emails\n",
    "\n",
    "file_path = r'..\\data\\email_exchanges_big.txt'\n",
    "\n",
    "emails_list = extract_emails(file_path)\n",
    "for address in emails_list:\n",
    "    print(address)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2: Most common words in English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(file_path, limit):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read().lower()\n",
    "        words = re.findall(r'\\b\\w+\\b', text)\n",
    "        word_count = Counter(words)\n",
    "        return word_count.most_common(limit)\n",
    "\n",
    "print('The ten most common words are:')\n",
    "print(find_most_common_words(file_path, 10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3a: Ten most frequent words in Obama's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def find_most_common_words(data, num_words):\n",
    "    words = re.findall(r'\\b\\w+\\b', data.lower())\n",
    "    word_counter = Counter(words)\n",
    "    common_words = [(word, freq) for word, freq in word_counter.items()]\n",
    "    common_words.sort(key=lambda x: x[1], reverse=True)\n",
    "    return common_words[:num_words]\n",
    "\n",
    "filename = '../data/obama_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "print(\"The 10 most repeated words in Obama's speech are:\")\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(f'{word} : {frequency} times')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3b: Ten most frequent words in Michelle's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/michelle_obama_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "print(\"The 10 most repeated words in Michelle's are:\")\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(f'{word} : {frequency} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3c: Ten most frequent words in Trump's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/donald_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "print(\"The 10 most repeated words in Trump's are:\")\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(f'{word} : {frequency} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number 3d: Ten most frequent words in Melina's speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '../data/melina_trump_speech.txt'\n",
    "with open(filename, 'r') as file:\n",
    "    speech_text = file.read()\n",
    "\n",
    "\n",
    "most_frequent_words = find_most_common_words(speech_text, 10)\n",
    "\n",
    "print(\"The 10 most frequent words in Melina's speech are:\")\n",
    "for word, frequency in most_frequent_words:\n",
    "    print(f'{word} : {frequency} times')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4: Similrities between two texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import difflib\n",
    "\n",
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    return ' '.join(cleaned_text.split()).lower()\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    with open(r'C:\\Users\\user\\Desktop\\ArewaDS\\ArewaDS-30days-of-Python\\data\\stop_words.py', 'r') as file:\n",
    "        stop_words = file.read().split(',')\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def check_text_similarity(text1, text2):\n",
    "    cleaned_text1 = remove_stop_words(clean_text(text1))\n",
    "    cleaned_text2 = remove_stop_words(clean_text(text2))\n",
    "    similarity_percentage = difflib.SequenceMatcher(None, cleaned_text1, cleaned_text2).ratio() * 100\n",
    "    return similarity_percentage\n",
    "\n",
    "text1 = r\"..\\data\\michelle_obama_speech.txt\" \n",
    "\n",
    "text2 = r\"..\\data\\melina_trump_speech.txt\" \n",
    "similarity = check_text_similarity(text1, text2)\n",
    "\n",
    "print(f' Similrities between two texts is {similarity}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5: Ten most repeated words in romeo and juliet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "with open('../data/romeo_and_juliet.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "content = re.sub(r'[^\\w\\s]', '', content.lower())\n",
    "\n",
    "words = content.split()\n",
    "\n",
    "word_counts = Counter(words)\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "print(\"The 10 most repeated words in 'romeo_and_juliet.txt' are:\")\n",
    "for word, count in top_10_words:\n",
    "    print(f\"{word}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6a: Number of lines containing python or Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def count_specific_words(csv_path, *words):\n",
    "    with open(csv_path, newline='') as file:\n",
    "        reader = csv.reader(file)\n",
    "        count = 0\n",
    "        for row in reader:\n",
    "            line = ' '.join(row).lower()\n",
    "            if all(word.lower() in line for word in words):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "python_count = count_specific_words(r'..\\data\\hacker_news.csv', 'python')\n",
    "\n",
    "print(f'Number of lines containing Java not JavaScript is: {python_count} lines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6b: Number of lines containing JavaScript, javascript or Javascript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "javascript_count = count_specific_words(r'..\\data\\hacker_news.csv', 'javascript', 'java')\n",
    "\n",
    "print(f'Number of lines containing JavaScript is: {javascript_count} lines')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6c: Number of lines containing Java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Java_count = count_specific_words(r'..\\data\\hacker_news.csv', 'Java')\n",
    "\n",
    "print(f'The number of lines containing JavaScript is: {Java_count} lines')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
